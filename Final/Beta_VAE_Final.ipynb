{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4eab30-0513-49d5-adb3-19f4e64e943f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; text-align:center; padding:40px;\">\n",
    "<h1  style=\"color:red;\" > MMI-714 : Generative Models for Multimedia </h1>   \n",
    "<h2  style=\"color:red;\" > Final Report </h2>\n",
    "<br>\n",
    "<h3  style=\"color:red;  font-style:italic;\" > Exploration of the Intuitive Physics through the \n",
    "Latent Space Disentanglement</h3>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Turgay Yıldız</h4>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Cognitive Sciences,  Middle East Technical University (METU)</h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8c966-c30a-4dde-9996-034d3c9cbb5a",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  First Import the Relevant Packages  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6616b3f-7539-4fcd-b3d0-674d33836324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.checkpoint import checkpoint \n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn.init as init\n",
    "from torchvision import models\n",
    "from torch.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import cv2\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6121e-3e4e-4739-80fb-c2694dc6d5cd",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Load the Dataset  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd3997-fe20-4eb2-84c9-52946201a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Data/6_pairs/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6e531-7d95-4edf-8215-cc8106a3e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = path + \"concatenated_data.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f7391-4a54-4e97-a10d-449e9614ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(path_data, mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4b208-3f82-4365-84e9-b1dfbbc1d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad73e11-bd62-4124-adfd-c33b65583b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =  X.reshape(3529, 6, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742abfe4-1f06-4ede-a047-f5c503bed003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image =  X[:, 0, :, :, :] \n",
    "X_color =  X[:, 1, :, :, :] \n",
    "X_order =  X[:, 2, :, :, :] \n",
    "\n",
    "X_image2 =  X[:, 3, :, :, :]\n",
    "X_color2 =  X[:, 4, :, :, :]\n",
    "X_order2 =  X[:, 5, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b3a9a-3a5d-41e6-90c1-295485082892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        \n",
    "        self.X     =    X\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_array   = np.array(self.X[idx], dtype=np.float32)       \n",
    "        img_tensor  = torch.tensor(img_array)                           \n",
    "\n",
    "\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fb6f4-ae91-4e8c-89a0-7996eabb6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(X_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306dd65-c99a-473f-817a-249f721132d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset   = Subset(dataset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46a442-6c5a-4d00-8983-cff635e57aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb87f3e-a508-443a-8334-14f0cb688f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfe7d2-d32b-48e9-a8cb-5ed84cfaba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i  =  i.reshape(-1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d934-4e6f-4c93-b54e-887e29071bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.min(), i.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413d55d-f5bc-49cc-bd56-7b83f42e6a71",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Function to Plot Images :  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dda515-288d-463f-a091-c25a9894ad6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img(data, row, col, size1, size2, c_map = None): \n",
    "    \n",
    "    if (row == 1) and (col == 1): \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(size1, size2))\n",
    "        ax.imshow(data, cmap=c_map) \n",
    "        ax.set_axis_off() \n",
    "\n",
    "    elif (row == 1) and (col > 1):\n",
    "        fig, axes = plt.subplots(1, col, figsize=(size1, size2))\n",
    "        for i in range(col):\n",
    "            axes[i].imshow(data[i], cmap=c_map) \n",
    "            axes[i].set_axis_off() \n",
    "            axes[i].set_title(f\"Image {i}\")\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(row, col, figsize=(size1, size2))\n",
    "        axes = axes.flatten() \n",
    "        for i in range(row * col):\n",
    "            if i < len(data):  \n",
    "                axes[i].imshow(data[i], cmap=c_map) \n",
    "                axes[i].set_axis_off()  \n",
    "                axes[i].set_title(f\"Image {i}\")\n",
    "            else:\n",
    "                axes[i].axis('off') \n",
    "\n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe422bb-57f6-4341-b9eb-e71c551ba15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04010559-a363-46f7-8d38-2ae55dc11a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_img(i[:12] / 255., 1, 12, 15, 15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e30424-d406-40c5-a028-fe252290e53d",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Model </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fefae-2f33-4a49-a098-d1dac0d9292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Beta_VAE(nn.Module):\n",
    "    def __init__(self, inp_out_dim=0, latent_dim=0, first_dim=0, dropout_prob=0.2):\n",
    "        super(Beta_VAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inp_out_dim = inp_out_dim\n",
    "        self.first_dim = first_dim\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------#\n",
    "        #-------------------------------------------           ENCODER               ------------------------------------------------#\n",
    "        #----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(self.inp_out_dim, self.first_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(self.first_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.first_dim, self.first_dim, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(self.first_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 112\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(self.first_dim, self.first_dim * 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(self.first_dim * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.first_dim * 2, self.first_dim * 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(self.first_dim * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 56\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(self.first_dim * 2, self.first_dim * 4, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(self.first_dim * 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(self.first_dim * 4, self.first_dim * 8, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(self.first_dim * 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(self.first_dim * 8, self.first_dim * 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(self.first_dim * 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 7\n",
    "\n",
    "            # Block 6\n",
    "            nn.Conv2d(self.first_dim * 16, self.first_dim * 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(self.first_dim * 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------#\n",
    "        #-------------------------------------------           FC_MU & LOG           ------------------------------------------------#\n",
    "        #----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.first_dim * 32 * 7 * 7, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(256, self.latent_dim),\n",
    "        )\n",
    "\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.first_dim * 32 * 7 * 7, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(256, self.latent_dim),\n",
    "        )\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------#\n",
    "        #-------------------------------------------           DECODER               ------------------------------------------------#\n",
    "        #----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "\n",
    "            nn.Linear(1024, self.first_dim * 128 * 7 * 7),\n",
    "            nn.BatchNorm1d(self.first_dim * 128 * 7 * 7),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.cnn_decoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(self.first_dim * 128, self.first_dim * 64, kernel_size=4, stride=2, padding=1),  # 7 -> 14\n",
    "            nn.BatchNorm2d(self.first_dim * 64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(self.first_dim * 64, self.first_dim * 32, kernel_size=4, stride=2, padding=1),  # 14 -> 28\n",
    "            nn.BatchNorm2d(self.first_dim * 32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(self.first_dim * 32, self.first_dim * 16, kernel_size=4, stride=2, padding=1),  # 28 -> 56\n",
    "            nn.BatchNorm2d(self.first_dim * 16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(self.first_dim * 16, self.first_dim * 8, kernel_size=4, stride=2, padding=1),  # 56 -> 112\n",
    "            nn.BatchNorm2d(self.first_dim * 8),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(self.first_dim * 8, self.first_dim * 2, kernel_size=4, stride=2, padding=1),  # 112 -> 224\n",
    "            nn.BatchNorm2d(self.first_dim * 2),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(self.first_dim * 2, self.inp_out_dim, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    #-------------------------------------------           FUNCTIONS               ----------------------------------------------#\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        \"\"\"\n",
    "        Initialize weights for linear and convolutional layers using Kaiming initialization.\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.cnn_encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decoder(z).reshape(-1, self.first_dim * 128, 7, 7)  # Use reshape instead of view\n",
    "        return self.cnn_decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=-1)\n",
    "        return reconstructed, kl_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0a82d-966c-4ccc-89ba-a0928d414d60",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Employ GPU </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e233f6-a9b1-41f9-80ef-07cd2756368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309790a-37c9-4097-b766-00f3d16aabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b628dd0-ace4-4abd-8468-8c4a1bf3d01e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Move Model to GPU  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89218f19-68c4-4e7e-82bc-90fcc784739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model   =   Beta_VAE(inp_out_dim = 3, latent_dim=16, first_dim = 4, dropout_prob = 0.2).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c189e0-4186-4036-9966-0dd037381dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f79d4-79a4-4522-acd2-663d6d01b45a",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Save Weights and Losses : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ff94-9bf5-4e36-ba65-becc198a0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model   =  \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Final_Report/codes/weights/Beta_VAE/Beta_VAE_weights_1.pth\"\n",
    "path_losses  =  \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Final_Report/codes/weights/Beta_VAE/Beta_VAE_losses_1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5235f-1ed2-4096-936d-882ab1b515e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss'       : 9999999999999,\n",
    "        }, path_model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135b3c2-dc1d-42fb-a09f-4cd8b3c868c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "            'train_loss_rec': [],\n",
    "            'train_loss_kl' : [],\n",
    "            'train_loss'    : [],\n",
    "    \n",
    "            'val_loss_rec'  : [],\n",
    "            'val_loss_kl'   : [],\n",
    "            'val_loss'      : [],\n",
    "    \n",
    "            'epochs'        : [],\n",
    "    \n",
    "        }, path_losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529781-d6ca-4e7e-a68c-a8abf8255a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_weights   =   torch.load(path_model, weights_only=True) \n",
    "checkpoint_losses    =   torch.load(path_losses, weights_only=True)  \n",
    "\n",
    "model.load_state_dict(checkpoint_weights['model_state_dict'])\n",
    "\n",
    "best_loss       =  checkpoint_weights['best_loss'] \n",
    "\n",
    "train_loss      =  checkpoint_losses['train_loss']\n",
    "train_loss_kl   =  checkpoint_losses['train_loss_kl']\n",
    "train_loss_rec  =  checkpoint_losses['train_loss_rec']\n",
    "\n",
    "val_loss        =  checkpoint_losses['val_loss'] \n",
    "val_loss_kl     =  checkpoint_losses['val_loss_kl'] \n",
    "val_loss_rec    =  checkpoint_losses['val_loss_rec'] \n",
    "\n",
    "epochs          =  checkpoint_losses['epochs'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666949b-5ee0-40d4-9b52-b82b797f4ef6",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Parameters :  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81c30a-c94d-4b45-9beb-43d9d7509a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "beta_start    =   0.0001\n",
    "beta_end      =   50\n",
    "\n",
    "num_epochs    =   100\n",
    "#------------------------------------\n",
    "mu_values     =   [] \n",
    "logvar_values =   []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaccca8-f8e1-4fb4-91c6-cd82b973f4dc",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Schedulers for Beta </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe86e4-ca46-4b03-b328-351bd8fc7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(x, start=0, end=10):\n",
    "    return start + (end - start) * (x / x.max())\n",
    "#---------------------------------------------------------------------------------#\n",
    "def exponential_schedule(epoch):\n",
    "    return beta_start * ((beta_end / beta_start) ** (epoch / num_epochs))\n",
    "#---------------------------------------------------------------------------------#\n",
    "def exponential_schedule_smooth(epoch=0, num_epochs=100, x_start=1, x_end=100):\n",
    "    # Interpolation factor using a sigmoid curve to ensure a smoother transition\n",
    "    scale = 1 / (1 + np.exp(-10 * (epoch / num_epochs - 0.5)))  # Sigmoid curve (0 to 1)\n",
    "    return x_start * (x_end / x_start) ** scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef0c98-2e7b-4757-8a8c-119a65d7934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 1000, 1), exponential_schedule_smooth(np.arange(0, 1000, 1), 1000, 1, 1000) / 20, \"r.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c89ff2-cc59-42a5-9fd5-62bcc2787f8d",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   Train and Validation Function </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581beb9-7c0b-4469-892e-f62accc72e7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(num_epochs=10, beta=0, patience=10, counter=0, learning_rate=0.001, best_loss=best_loss):\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler    = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        beta  =  exponential_schedule_smooth(epoch=epoch, num_epochs=250, x_start=1, x_end=250) / 5\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        total_loss_train      =   0.0\n",
    "        total_loss_train_kl   =   0.0\n",
    "        total_loss_train_rec  =   0.0\n",
    "\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            images = batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                \n",
    "                img_recon, kl_loss = model(images)\n",
    "                img_rec_loss       = criterion(img_recon, images)\n",
    "                loss               = img_rec_loss + kl_loss * beta\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss_train     += loss.item()\n",
    "            total_loss_train_kl  += kl_loss.item()\n",
    "            total_loss_train_rec += img_rec_loss.item()\n",
    "\n",
    "        total_loss_train     /= len(train_loader)\n",
    "        total_loss_train_kl  /= len(train_loader)\n",
    "        total_loss_train_rec /= len(train_loader)\n",
    "\n",
    "        train_loss.append(total_loss_train)\n",
    "        train_loss_kl.append(total_loss_train_kl)\n",
    "        train_loss_rec.append(total_loss_train_rec)\n",
    "\n",
    "        if torch.isnan(torch.tensor(total_loss_train)):\n",
    "            print(\"NaN value encountered!\")\n",
    "            break\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss : {total_loss_train:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total KL Loss : {total_loss_train_kl:.4f}              |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total REC Loss : {total_loss_train_rec:.4f}              |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_loss_val      =   0.0\n",
    "        total_loss_val_kl   =   0.0\n",
    "        total_loss_val_rec  =   0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                images = batch.to(device)\n",
    "\n",
    "                img_recon, kl_loss = model(images)\n",
    "                img_rec_loss       = criterion(img_recon, images)\n",
    "                loss               = img_rec_loss + kl_loss * beta\n",
    "\n",
    "                total_loss_val     += loss.item()\n",
    "                total_loss_val_kl  += kl_loss.item()\n",
    "                total_loss_val_rec += img_rec_loss.item()\n",
    "\n",
    "        total_loss_val     /= len(val_loader)\n",
    "        total_loss_val_kl  /= len(val_loader)\n",
    "        total_loss_val_rec /= len(val_loader)\n",
    "\n",
    "        val_loss.append(total_loss_val)\n",
    "        val_loss_kl.append(total_loss_val_kl)\n",
    "        val_loss_rec.append(total_loss_val_rec)\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total Validation Loss : {total_loss_val:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total KL Loss : {total_loss_val_kl:.4f}              |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total REC Loss : {total_loss_val_rec:.4f}             |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if len(val_loss) >= 2:\n",
    "            res = ((val_loss[-2] - val_loss[-1]) / val_loss[-2]) * 100\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "            print(f\"|              Change in loss is      %   {res:.2f}                               |\")\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if  total_loss_val_rec < best_loss:\n",
    "            \n",
    "            print(\"*************...saving best model *************\")\n",
    "            \n",
    "            best_loss = total_loss_val_rec\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_loss'       : best_loss,\n",
    "            }, path_model)\n",
    "             \n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        torch.save({\n",
    "            'train_loss_rec': train_loss_rec,\n",
    "            'train_loss_kl': train_loss_kl,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss_rec': val_loss_rec,\n",
    "            'val_loss_kl': val_loss_kl,\n",
    "            'val_loss': val_loss,\n",
    "            'epochs': epochs,\n",
    "        }, path_losses)\n",
    "\n",
    "        if (len(val_loss) >= 2) and (val_loss[-2] > val_loss[-1]):\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()  \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d0a23-c2e5-415c-9802-2383e0681ff6",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   Start Train and Validation </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faba81-7246-495b-9e34-48a10305596d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(num_epochs=250, beta=0, patience=10, counter=0, learning_rate=0.001 ,  best_loss=550)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592b007-3a31-49b0-b83d-a4173851ee66",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Plot Train and Validation Losses </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e905246-3e60-4882-b807-bbc5c71fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "ax.plot(train_loss_kl, \"b-\", label=\"Train Loss KL\")\n",
    "ax.plot(val_loss_kl, \"r-\", label=\"Validation Loss KL\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"KL_Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90aad7-dd0b-4635-9485-5c82da6fece2",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   To Test Reconstruction Quality </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7a7e1-8c02-4218-9cdd-44733d9aead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for img in val_loader:\n",
    "        img    = img.to(device)\n",
    "\n",
    "        recon, kl  = model(img)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccf2ec-a548-455e-9993-301cc9dd5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89924064-d065-48c2-b58f-25cf99d4a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.max(), recon.min(), recon.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085b78d-0b65-450d-9ae5-ee671205580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "\n",
    "fig, ax = plt.subplots(2, 7, figsize=(15, 4))\n",
    "\n",
    "for i in range(7):\n",
    "\tax[0, i].imshow( (img[i]/255.).reshape(224,224,3).cpu().numpy())\n",
    "\tax[1, i].imshow( (recon[i]/255.).reshape(224, 224, 3).cpu().numpy())\n",
    "\tax[0, i].axis('OFF')\n",
    "\tax[1, i].axis('OFF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99986ee0-ed01-44fe-8df9-23ba78c3ddce",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   Test Latent Space </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c121ea8-67e3-4547-b80f-fa252c430027",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f37bc-bc2a-4815-80b5-466aea590309",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:1px;\">\n",
    "<h2>   The following image will be sent to the latent space: </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739986a-5cfe-46f9-af8d-edc28d743481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(dpi=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.imshow((img[5]/255.).reshape(224,224,3).cpu().numpy())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69076932-e4f5-4fb6-8c20-f39ff393335e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:1px;\">\n",
    "<h2>   Create Vector z : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d395f1-9e41-46f6-a265-8405dfae0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logvar = model.encode(img[5].reshape(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19fff8-0837-4449-88ce-75d3c8ddd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.reparameterize(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91e0a5-8f25-49fe-847f-27df52fd6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bccec-109a-4e6f-a5d6-7dea3fe92a3a",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:1px;\">\n",
    "<h2>   Traversals Function : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51c1ae-757d-4e79-8cdf-271982965190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_traversal(model, z, latent_dim, steps=10, range_val=3):\n",
    "\n",
    "    device = next(model.parameters()).device  \n",
    "\n",
    "    fig, axs = plt.subplots(latent_dim, steps, figsize=(steps * 2, latent_dim * 2))\n",
    "\n",
    "    for dim in range(latent_dim):\n",
    "        for step, val in enumerate(torch.linspace(-range_val, range_val, steps)):\n",
    " \n",
    "            z_traversal             =   z.clone()\n",
    "            z_traversal[0, dim]     =   val \n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                recon              = model.decode(z_traversal).squeeze(0).cpu().numpy()\n",
    "\n",
    "            axs[dim, step].imshow((recon/255.).reshape(224, 224, 3))\n",
    "            axs[dim, step].axis(\"off\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa877515-1682-46f1-a318-0fc3a60571e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_space_traversal(model, z, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592a3b2-7ad4-42f4-9a88-ccbb849b719d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f7b9c-d99c-49c2-bdff-223db2b6d191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
