{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4eab30-0513-49d5-adb3-19f4e64e943f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; text-align:center; padding:40px;\">\n",
    "<h1  style=\"color:red;\" > MMI-714 : Generative Models for Multimedia </h1>   \n",
    "<h2  style=\"color:red;\" > Final Report </h2>\n",
    "<br>\n",
    "<h3  style=\"color:red;  font-style:italic;\" > Exploration of the Intuitive Physics through the \n",
    "Latent Space Disentanglement</h3>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Turgay Yıldız</h4>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Cognitive Sciences,  Middle East Technical University (METU)</h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8c966-c30a-4dde-9996-034d3c9cbb5a",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  First Import the Relevant Packages  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6616b3f-7539-4fcd-b3d0-674d33836324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.checkpoint import checkpoint \n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn.init as init\n",
    "from torchvision import models\n",
    "from torch.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6121e-3e4e-4739-80fb-c2694dc6d5cd",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Load the Dataset  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba768f-f748-47fd-849a-945e577e22c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        \n",
    "        self.img_dir     =    img_dir\n",
    "        \n",
    "        self.img_files   =    sorted(os.listdir(img_dir))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "\n",
    "        img         = Image.open(img_path).convert(\"RGB\")  # Ensure 3-channel RGB\n",
    "        img_array   = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        img_tensor  = torch.tensor(img_array).reshape(3, 224, 224)  # Convert to PyTorch tensor and reshape to (C, H, W)\n",
    "\n",
    "\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b723c7-4d22-4d86-a6cf-f29ea9008e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_path = \"/home/turgay/falling_objects_dataset/img_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bda1f0-89d1-42d1-9866-8ac4a037da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(img_path))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebebff-775a-4b28-a089-6e0b46183998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ImageDataset(img_dir=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef46fa-55f0-4ad0-a426-90c883e20f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06fca6-ad57-49bc-bb59-4771bfb4b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size =  int(0.8 * len(dataset))\n",
    "val_size   =  len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39a958-3da3-4399-8b9c-9bbf22b26c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fccac7-fdf6-47bc-86d2-03d7ac6e6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc62d95-1eef-479c-96e6-7e74c1eb56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b88ad8-970a-420d-8863-c220c016b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.max(), i.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebf11f-ed32-4913-ad45-50f4c3ce67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "i  =  i.reshape(-1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a55c88-e262-44e0-bf98-20ade1517889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd9069-42bd-4130-ac85-6babbbe5d5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd3997-fe20-4eb2-84c9-52946201a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Data/6_pairs/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6e531-7d95-4edf-8215-cc8106a3e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = path + \"concatenated_data.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f7391-4a54-4e97-a10d-449e9614ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(path_data, mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4b208-3f82-4365-84e9-b1dfbbc1d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad73e11-bd62-4124-adfd-c33b65583b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =  X.reshape(3529, 6, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742abfe4-1f06-4ede-a047-f5c503bed003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image =  X[:, 0, :, :, :] \n",
    "X_color =  X[:, 1, :, :, :] \n",
    "X_order =  X[:, 2, :, :, :] \n",
    "\n",
    "X_image2 =  X[:, 3, :, :, :]\n",
    "X_color2 =  X[:, 4, :, :, :]\n",
    "X_order2 =  X[:, 5, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df325e5-310e-45d4-878f-5fefe4eb448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b3a9a-3a5d-41e6-90c1-295485082892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        \n",
    "        self.X     =    X\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_array   = np.array(self.X[idx], dtype=np.float32) / 255.0         # Normalize to [0, 1]\n",
    "        img_tensor  = torch.tensor(img_array)                            # Convert to PyTorch tensor and reshape to (C, H, W)\n",
    "\n",
    "\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fb6f4-ae91-4e8c-89a0-7996eabb6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(X_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219fe45-1eb5-43dc-94a2-0b9629390357",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109e139-3115-4bdb-a5fa-a89591007683",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size =  int(0.8 * len(dataset))\n",
    "val_size   =  len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986171f-f28d-4210-8a78-f8a48aadd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46a442-6c5a-4d00-8983-cff635e57aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb87f3e-a508-443a-8334-14f0cb688f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfe7d2-d32b-48e9-a8cb-5ed84cfaba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i  =  i.reshape(-1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413d55d-f5bc-49cc-bd56-7b83f42e6a71",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Function to Plot Images :  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dda515-288d-463f-a091-c25a9894ad6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img(data, row, col, size1, size2, c_map = None): \n",
    "    \n",
    "    if (row == 1) and (col == 1): \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(size1, size2))\n",
    "        ax.imshow(data, cmap=c_map) \n",
    "        ax.set_axis_off() \n",
    "\n",
    "    elif (row == 1) and (col > 1):\n",
    "        fig, axes = plt.subplots(1, col, figsize=(size1, size2))\n",
    "        for i in range(col):\n",
    "            axes[i].imshow(data[i], cmap=c_map) \n",
    "            axes[i].set_axis_off() \n",
    "            axes[i].set_title(f\"Image {i}\")\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(row, col, figsize=(size1, size2))\n",
    "        axes = axes.flatten() \n",
    "        for i in range(row * col):\n",
    "            if i < len(data):  \n",
    "                axes[i].imshow(data[i], cmap=c_map) \n",
    "                axes[i].set_axis_off()  \n",
    "                axes[i].set_title(f\"Image {i}\")\n",
    "            else:\n",
    "                axes[i].axis('off') \n",
    "\n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04010559-a363-46f7-8d38-2ae55dc11a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_img(i[:10], 2, 5, 5, 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ccf69-f8f7-443c-a647-e42b9a9ee76a",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Patch Embeddings for the ViT part of the architecture :  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923641df-2e9e-44cd-881c-72c8bcadc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, patch_size = 16, embed_dim = 64):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfd61b-cb81-4787-a37a-80e845b94538",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Multi-Head Attention  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2fc4b-2b70-44ed-9ad0-42eea0555420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads   =    n_heads\n",
    "        self.att       =    torch.nn.MultiheadAttention(embed_dim =  embed_dim,\n",
    "                                                       num_heads  =  n_heads,\n",
    "                                                       dropout    =  dropout ) \n",
    "        self.q = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = torch.nn.Linear(embed_dim, embed_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.att(x, x, x)\n",
    "        \n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f998f-ef6e-4444-bccd-558a54dd47a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention(embed_dim=256, n_heads=4, dropout=0.)(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8e218-0bf2-445f-a9ce-debc0a948f5d",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Normalizing Module : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b284f-b6ce-4549-85cf-8f7d4f29afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm   =   nn.LayerNorm(embed_dim)\n",
    "        self.fn     =   fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        \n",
    "        return self.fn(self.norm(x), **kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc39cae-9660-4e81-a53a-efd399e8c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm = PreNorm(embed_dim=256, fn=Attention(embed_dim=256, n_heads=4, dropout=0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8902f6-7e88-40e4-b20b-7d6ec786c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dbb61-2a4b-4585-938d-e0436c9dfd42",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Feed-Forward  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07dce0-2f8b-4272-adae-894fc75f9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, dropout = 0.):    \n",
    "        \n",
    "        super().__init__(\n",
    "            \n",
    "            nn.Linear(embed_dim, hidden_dim),       #   hidden_dim   =   2   *   embed_dim\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),       \n",
    "            nn.Dropout(dropout) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9e24c-51c9-434e-8e5c-9dfffa493011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff = FeedForward(embed_dim=256, hidden_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4ccc5-dcbf-4eaf-9ddc-2a57a1491762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33aa55-c32a-4c6c-badd-14c83269e7bd",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Residual Connections </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654df1fb-e69e-4788-862d-e415cc75e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \n",
    "        res   =   x\n",
    "        x     =   self.fn(x, **kwargs)\n",
    "        x    +=   res\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1d03c-f1f5-4fa0-99eb-362c4a109046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residual_att = ResidualAdd(Attention(embed_dim=256, n_heads=4, dropout=0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46ac57-905f-43df-b8ae-56ec648ec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residual_att(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e30424-d406-40c5-a028-fe252290e53d",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Model </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fefae-2f33-4a49-a098-d1dac0d9292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beta_VAE_ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch=3, img_size=224, patch_size=16, embed_dim=64, latent_dim = 64, n_layers=6, dropout=0.1, heads=8):\n",
    "        super(Beta_VAE_ViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels    =   ch\n",
    "        self.height      =   img_size\n",
    "        self.width       =   img_size\n",
    "        self.patch_size  =   patch_size\n",
    "        self.n_layers    =   n_layers\n",
    "        self.embed_dim   =   embed_dim\n",
    "        self.latent_dim  =   latent_dim\n",
    "        \n",
    "        \n",
    "        num_patches      =   (img_size // patch_size) ** 2\n",
    "        self.num_patches =   num_patches\n",
    "        \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        # Patching\n",
    "        self.patch_embedding  = PatchEmbedding(in_channels =  ch,\n",
    "                                               patch_size  =  patch_size,\n",
    "                                               embed_dim   =  self.embed_dim)\n",
    "#-----------------------------------------------------------------------------------------------------------        \n",
    "        self.pos_embedding    =    nn.Parameter(torch.randn(1, num_patches + 1, self.embed_dim))\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        # Transformer Encoder\n",
    "        self.encoder_layers   =    nn.ModuleList([]) \n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            transformer_encoder_block = nn.Sequential(\n",
    "                                            ResidualAdd(PreNorm(self.embed_dim, Attention(self.embed_dim, n_heads = heads, dropout = dropout))),\n",
    "                                            ResidualAdd(PreNorm(self.embed_dim, FeedForward(self.embed_dim, 2 * self.embed_dim, dropout = dropout)))\n",
    "                                            )\n",
    "            self.encoder_layers.append(transformer_encoder_block) #  (self.num_patches,   self.embed_dim)  \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        self.fc_encoder_mu = nn.Sequential(\n",
    "                                        \n",
    "                                        nn.Linear(self.num_patches * self.embed_dim,   512),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(512),\n",
    "                                         \n",
    "                                        nn.Linear(512, 256),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(256),\n",
    "            \n",
    "                                        nn.Linear(256,  self.latent_dim),\n",
    "                                \n",
    "                                        )\n",
    "    \n",
    "        self.fc_encoder_logvar = nn.Sequential(\n",
    "            \n",
    "                                        nn.Linear(self.num_patches * self.embed_dim,   512),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(512),\n",
    "                                         \n",
    "                                        nn.Linear(512, 256),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(256),\n",
    "            \n",
    "                                        nn.Linear(256,  self.latent_dim),\n",
    "                                     \n",
    "                                        )\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            \n",
    "                                        nn.BatchNorm1d(self.latent_dim),\n",
    "            \n",
    "                                        nn.Linear(self.latent_dim,         num_patches * 4),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(num_patches * 4),\n",
    "            \n",
    "                                        nn.Linear(num_patches * 4,         num_patches * 16),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(num_patches * 16),\n",
    "            \n",
    "                                        nn.Linear(num_patches * 16,        num_patches * 64),   #  196 * 64\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(num_patches * 64),\n",
    "                                        )\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        self.reconstruct = nn.Sequential(\n",
    "         \n",
    "                nn.ConvTranspose2d(self.num_patches, 128, kernel_size=4, stride=2, padding=2),         #   8x8   ->  14x14 \n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            \n",
    "                nn.ConvTranspose2d(128,          64, kernel_size=4, stride=2, padding=1),              #   14    ->  28\n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            \n",
    "                nn.ConvTranspose2d(64,           32, kernel_size=4, stride=2, padding=1),              #   28    ->  56\n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            \n",
    "                nn.ConvTranspose2d(32,           16, kernel_size=4, stride=2, padding=1),              #   56    ->  112\n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            \n",
    "                nn.ConvTranspose2d(16,           3,  kernel_size=4, stride=2, padding=1),               #   112   ->  224\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "#-------------------------------------------           FUNCTIONS               ----------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "        x          =   self.patch_embedding(x)\n",
    "        b, n, _    =   x.shape \n",
    "\n",
    "        x         +=   self.pos_embedding[:, : n]\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encoder_layers[i](x)\n",
    "            \n",
    "        x      =   x.reshape(-1, self.num_patches * self.embed_dim)\n",
    "            \n",
    "        mu     = self.fc_encoder_mu(x)\n",
    "        logvar = self.fc_encoder_logvar(x) \n",
    "        \n",
    "        return mu , logvar\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) \n",
    "        z   = mu + eps * std \n",
    "        \n",
    "        return z  \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "      \n",
    "    def decode(self, z):\n",
    "        \n",
    "            x      =   self.fc_decoder(z)\n",
    "            x      =   x.reshape(-1, self.num_patches, 8, 8)\n",
    "            x      =   self.reconstruct(x)\n",
    "            \n",
    "            return x\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar        =    self.encode(x)\n",
    "        z                 =    self.reparameterize(mu, logvar)\n",
    "        reconstructed     =    self.decode(z)  \n",
    "        \n",
    "        kl_loss           =    -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=-1)\n",
    "        \n",
    "        return reconstructed, kl_loss.mean()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4e6ac-83ff-4862-8686-6f71b3b15fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(model):\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "\n",
    "            # Convolutional layers\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')  # Kaiming initialization for conv layers\n",
    "                if module.bias is not None:\n",
    "                    init.zeros_(module.bias)  # Initialize biases to zero\n",
    "\n",
    "            # Transposed Convolutional layers (Deconvolution)\n",
    "            elif isinstance(module, nn.ConvTranspose2d):\n",
    "                init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')  # Kaiming for transpose conv\n",
    "                if module.bias is not None:\n",
    "                    init.zeros_(module.bias)\n",
    "\n",
    "            # Fully connected (Linear) layers\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                init.xavier_normal_(module.weight)  # Xavier initialization for FC layers\n",
    "                if module.bias is not None:\n",
    "                    init.zeros_(module.bias)  # Initialize biases to zero\n",
    "\n",
    "            # BatchNorm layers (if any)\n",
    "            elif isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n",
    "                init.ones_(module.weight)  # Initialize BatchNorm scale parameter to 1\n",
    "                init.zeros_(module.bias)  # Initialize BatchNorm bias parameter to 0\n",
    "\n",
    "            # Transformer layers (self-attention and feedforward)\n",
    "            elif isinstance(module, nn.TransformerEncoderLayer):\n",
    "                # Initialize self-attention layers\n",
    "                init.xavier_normal_(module.self_attn.in_proj_weight)  # Attention projection weights\n",
    "                init.xavier_normal_(module.self_attn.out_proj.weight)  # Attention output projection weights\n",
    "                if module.self_attn.in_proj_bias is not None:\n",
    "                    init.zeros_(module.self_attn.in_proj_bias)  # Initialize biases to zero\n",
    "\n",
    "                # Initialize feedforward layers (MLP within the Transformer)\n",
    "                init.kaiming_normal_(module.linear1.weight, mode='fan_in', nonlinearity='relu')  # Kaiming for the first FC layer\n",
    "                init.kaiming_normal_(module.linear2.weight, mode='fan_in', nonlinearity='relu')  # Kaiming for the second FC layer\n",
    "                if module.linear1.bias is not None:\n",
    "                    init.zeros_(module.linear1.bias)  # Initialize biases to zero\n",
    "                if module.linear2.bias is not None:\n",
    "                    init.zeros_(module.linear2.bias)\n",
    "\n",
    "            # Transformer Encoder\n",
    "            elif isinstance(module, nn.TransformerEncoder):\n",
    "                # Initialize all transformer encoder layers\n",
    "                for encoder_layer in module.layers:\n",
    "                    weights_init(encoder_layer)  # Recursively initialize each transformer encoder layer\n",
    "\n",
    "            # For any other modules, initialize as needed\n",
    "            else:\n",
    "                pass  # Default behavior for modules that don't require special initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3650044-9b35-455d-bcf7-22af6a2028f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97a0a82d-966c-4ccc-89ba-a0928d414d60",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Employ GPU </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e233f6-a9b1-41f9-80ef-07cd2756368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309790a-37c9-4097-b766-00f3d16aabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b628dd0-ace4-4abd-8468-8c4a1bf3d01e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Move Model to GPU  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89218f19-68c4-4e7e-82bc-90fcc784739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model   =   Beta_VAE_ViT(ch=3, img_size=224, patch_size=16, embed_dim=64, latent_dim = 64, n_layers=6, dropout=0.1, heads=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c189e0-4186-4036-9966-0dd037381dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a265e-2092-43ab-bc82-4c048de40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f79d4-79a4-4522-acd2-663d6d01b45a",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Save Weights and Losses : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ff94-9bf5-4e36-ba65-becc198a0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model   =  \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Final_Report/codes/weights/Beta_VAE_transformers_weights_1.pth\"\n",
    "path_losses  =  \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Final_Report/codes/weights/Beta_VAE_transformers_losses_1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5235f-1ed2-4096-936d-882ab1b515e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss'       : 9999999999999,\n",
    "        }, path_model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135b3c2-dc1d-42fb-a09f-4cd8b3c868c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'train_loss_rec': [],\n",
    "            'train_loss_kl' : [],\n",
    "            'train_loss'    : [],\n",
    "    \n",
    "            'val_loss_rec'  : [],\n",
    "            'val_loss_kl'   : [],\n",
    "            'val_loss'      : [],\n",
    "    \n",
    "            'epochs'        : [],\n",
    "    \n",
    "        }, path_losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529781-d6ca-4e7e-a68c-a8abf8255a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_weights   =   torch.load(path_model, weights_only=True) \n",
    "checkpoint_losses    =   torch.load(path_losses, weights_only=True)  \n",
    "\n",
    "model.load_state_dict(checkpoint_weights['model_state_dict'])\n",
    "\n",
    "best_loss       =  checkpoint_weights['best_loss'] \n",
    "\n",
    "train_loss      =  checkpoint_losses['train_loss']\n",
    "train_loss_kl   =  checkpoint_losses['train_loss_kl']\n",
    "train_loss_rec  =  checkpoint_losses['train_loss_rec']\n",
    "\n",
    "val_loss        =  checkpoint_losses['val_loss'] \n",
    "val_loss_kl     =  checkpoint_losses['val_loss_kl'] \n",
    "val_loss_rec    =  checkpoint_losses['val_loss_rec'] \n",
    "\n",
    "epochs          =  checkpoint_losses['epochs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc54c1-6c67-4fa2-9639-74b25f24ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666949b-5ee0-40d4-9b52-b82b797f4ef6",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Parameters :  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81c30a-c94d-4b45-9beb-43d9d7509a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "beta_start    =   0.000001\n",
    "beta_end      =   10\n",
    "\n",
    "num_epochs    =   100\n",
    "#------------------------------------\n",
    "mu_values     =   [] \n",
    "logvar_values =   []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaccca8-f8e1-4fb4-91c6-cd82b973f4dc",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Schedulers for Beta </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe86e4-ca46-4b03-b328-351bd8fc7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(epoch):\n",
    "    return beta_start + ((beta_end - beta_start) / num_epochs) * epoch \n",
    "#---------------------------------------------------------------------------------#\n",
    "def exponential_schedule(epoch):\n",
    "    return beta_start * ((beta_end / beta_start) ** (epoch / num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef0c98-2e7b-4757-8a8c-119a65d7934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15c89ff2-cc59-42a5-9fd5-62bcc2787f8d",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   Train and Validation Function </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581beb9-7c0b-4469-892e-f62accc72e7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(num_epochs=10, beta=0, patience=10, counter=0, learning_rate=0.001, best_loss=99999999 ):\n",
    "    \n",
    "    \n",
    "    optimizer =   torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion =   nn.MSELoss()\n",
    "    scaler    =   GradScaler() \n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        beta   =   beta \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_loss_train      =   0.0\n",
    "        total_loss_train_kl   =   0.0\n",
    "        total_loss_train_rec  =   0.0\n",
    "\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "\n",
    "            images   = batch\n",
    "\n",
    "            images   =  images.to(device)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "\n",
    "                img_recon, kl_loss   =   model(images)\n",
    "\n",
    "                img_rec_loss         =   criterion(img_recon, images)\n",
    "\n",
    "                loss = img_rec_loss + kl_loss * beta  \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "            scaler.scale(loss).backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss_train      +=   loss.item()\n",
    "            total_loss_train_kl   +=   kl_loss.item()\n",
    "            total_loss_train_rec  +=   img_rec_loss.item()\n",
    "\n",
    "        total_loss_train     /= len(train_loader)\n",
    "        total_loss_train_kl  /= len(train_loader)\n",
    "        total_loss_train_rec /= len(train_loader)\n",
    "\n",
    "        train_loss.append(total_loss_train)\n",
    "        train_loss_kl.append(total_loss_train_kl)\n",
    "        train_loss_rec.append(total_loss_train_rec)\n",
    "\n",
    "        total_loss_train_tensor = torch.tensor(total_loss_train)\n",
    "\n",
    "        if torch.isnan(total_loss_train_tensor):\n",
    "            print(\"nan value is encountered !\")\n",
    "\n",
    "            break\n",
    "\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss : {total_loss_train:.4f}           |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total KL Loss : {total_loss_train_kl:.4f}              |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total REC Loss : {total_loss_train_rec:.4f}              |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_loss_val      =   0.0\n",
    "        total_loss_val_kl   =   0.0\n",
    "        total_loss_val_rec  =   0.0\n",
    "        total_loss_val_reg  =   0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "                images   =  batch\n",
    "\n",
    "                images   =  images.to(device)\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "                img_recon, kl_loss   =   model(images)\n",
    "\n",
    "                img_rec_loss         =   criterion(img_recon, images)\n",
    "\n",
    "                loss = img_rec_loss + kl_loss * beta  \n",
    "\n",
    "\n",
    "                total_loss_val      +=   loss.item()\n",
    "                total_loss_val_kl   +=   kl_loss.item()\n",
    "                total_loss_val_rec  +=   img_rec_loss.item()\n",
    "\n",
    "\n",
    "        total_loss_val     /= len(val_loader)\n",
    "        total_loss_val_kl  /= len(val_loader)\n",
    "        total_loss_val_rec /= len(val_loader)\n",
    "\n",
    "        val_loss.append(total_loss_val)\n",
    "        val_loss_kl.append(total_loss_val_kl)\n",
    "        val_loss_rec.append(total_loss_val_rec)\n",
    "\n",
    "\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total Validation Loss : {total_loss_val:.4f}           |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total KL Loss : {total_loss_val_kl:.4f}              |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total REC Loss : {total_loss_val_rec:.4f}             |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        if len(val_loss) >= 2:\n",
    "\n",
    "            res   =   (  (val_loss[-2] - val_loss[-1]) / val_loss[-2] ) * 100 \n",
    "            print( \"-------------------------------------------------------------------------------\")\n",
    "            print(f\"|              Change in loss is      %   {res:.2f}                               |\")\n",
    "            print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if total_loss_val < best_loss:\n",
    "            print(\"*************...saving best model *************\")\n",
    "            best_loss = total_loss_val \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "            }, path_model)   \n",
    "\n",
    "        epochs.append(epoch)\n",
    "        \n",
    "        torch.save({\n",
    "                'train_loss_rec'   : train_loss_rec,\n",
    "                'train_loss_kl'    : train_loss_kl,\n",
    "                'train_loss'       : train_loss,\n",
    "\n",
    "                'val_loss_rec'     : val_loss_rec,\n",
    "                'val_loss_kl'      : val_loss_kl,\n",
    "                'val_loss'         : val_loss,\n",
    "\n",
    "                'epochs'           : epochs, \n",
    "\n",
    "            }, path_losses) \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "        if (len(val_loss) >= 2) and (val_loss[-2] > val_loss[-1]):\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "            \n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d0a23-c2e5-415c-9802-2383e0681ff6",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   Start Train and Validation </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faba81-7246-495b-9e34-48a10305596d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(num_epochs=100, beta=0, patience=10, counter=0, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592b007-3a31-49b0-b83d-a4173851ee66",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Plot Train and Validation Losses </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e905246-3e60-4882-b807-bbc5c71fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "ax.plot(train_loss, \"b-\", label=\"Train Loss\")\n",
    "ax.plot(val_loss, \"r-\", label=\"Validation Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"MSE_Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90aad7-dd0b-4635-9485-5c82da6fece2",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   To Test Reconstruction Quality </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7a7e1-8c02-4218-9cdd-44733d9aead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for img in val_loader:\n",
    "        img    = img.to(device)\n",
    "\n",
    "        recon, kl  = model(img)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccf2ec-a548-455e-9993-301cc9dd5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89924064-d065-48c2-b58f-25cf99d4a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.max(), recon.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085b78d-0b65-450d-9ae5-ee671205580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "\n",
    "fig, ax = plt.subplots(2, 7, figsize=(15, 4))\n",
    "\n",
    "for i in range(7):\n",
    "\tax[0, i].imshow(img[i].reshape(224,224,3).cpu().numpy())\n",
    "\tax[1, i].imshow(recon[i].reshape(224, 224, 3).cpu().numpy())\n",
    "\tax[0, i].axis('OFF')\n",
    "\tax[1, i].axis('OFF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99986ee0-ed01-44fe-8df9-23ba78c3ddce",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>   Test Latent Space </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c121ea8-67e3-4547-b80f-fa252c430027",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f37bc-bc2a-4815-80b5-466aea590309",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:1px;\">\n",
    "<h2>   The following image will be sent to the latent space: </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739986a-5cfe-46f9-af8d-edc28d743481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(dpi=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.imshow(img[1].reshape(224,224,3).cpu().numpy())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2ac33-1db5-4556-9db2-568c8aa935c6",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:1px;\">\n",
    "<h2>   Create Vector z: </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d395f1-9e41-46f6-a265-8405dfae0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logvar = model.encode(img[1].reshape(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19fff8-0837-4449-88ce-75d3c8ddd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.reparameterize(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91e0a5-8f25-49fe-847f-27df52fd6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c71692-2d02-4376-9e38-970e7ea5757c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85077c74-b665-4593-8de2-aabf14d270fd",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:1px;\">\n",
    "<h2>   Traversal Function: </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51c1ae-757d-4e79-8cdf-271982965190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_traversal(model, z, latent_dim, steps=10, range_val=3):\n",
    "\n",
    "    device = next(model.parameters()).device  \n",
    "\n",
    "    fig, axs = plt.subplots(latent_dim, steps, figsize=(steps * 2, latent_dim * 2))\n",
    "\n",
    "    for dim in range(latent_dim):\n",
    "        for step, val in enumerate(torch.linspace(-range_val, range_val, steps)):\n",
    " \n",
    "            z_traversal             =   z.clone()\n",
    "            z_traversal[0, dim]  =   val \n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                recon              = model.decode(z_traversal).squeeze(0).cpu().numpy()\n",
    "\n",
    "            axs[dim, step].imshow(recon.reshape(224, 224, 3))\n",
    "            axs[dim, step].axis(\"off\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa877515-1682-46f1-a318-0fc3a60571e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_space_traversal(model, z, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592a3b2-7ad4-42f4-9a88-ccbb849b719d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
