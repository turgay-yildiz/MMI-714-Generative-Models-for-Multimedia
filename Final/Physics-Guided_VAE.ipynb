{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776c71b7-4b33-4b3c-8861-7aad9c06e5c4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; text-align:center; padding:40px;\">\n",
    "<h1  style=\"color:red;\" > MMI-714 : Generative Models for Multimedia </h1>   \n",
    "<h2  style=\"color:red;\" > Final Report </h2>\n",
    "<br>\n",
    "<h3  style=\"color:red;  font-style:italic;\" > Exploration of the Intuitive Physics through the \n",
    "Latent Space Disentanglement</h3>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Turgay Yıldız</h4>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Cognitive Sciences,  Middle East Technical University (METU)</h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6616b3f-7539-4fcd-b3d0-674d33836324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.checkpoint import checkpoint \n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "from torchvision import models\n",
    "from torch.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import pygame\n",
    "import pymunk\n",
    "from pymunk import pygame_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d249b-4b03-4919-8354-8acdf2a7eab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#path = \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Data/6_pairs/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1b7af-70b8-417a-8e01-08a42d6074cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_data = path + \"concatenated_data.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fdc6f-5890-474c-8a88-361eda103907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.load(path_data, mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936b21e-8bfc-4d74-ad22-0d8ea306758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =  X.reshape(3529, 6, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4731ca-e86e-4e3c-8651-6c74b158c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image =  X[:, 0, :, :, :] \n",
    "X_color =  X[:, 1, :, :, :] \n",
    "X_order =  X[:, 2, :, :, :] \n",
    "\n",
    "X_image2 =  X[:, 3, :, :, :]\n",
    "X_color2 =  X[:, 4, :, :, :]\n",
    "X_order2 =  X[:, 5, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94f4be-d114-4c5b-b0f3-20683f9e7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dda515-288d-463f-a091-c25a9894ad6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img(data, row, col, size1, size2, c_map = None): \n",
    "    \n",
    "    if (row == 1) and (col == 1): \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(size1, size2))\n",
    "        ax.imshow(data, cmap=c_map) \n",
    "        ax.set_axis_off() \n",
    "\n",
    "    elif (row == 1) and (col > 1):\n",
    "        fig, axes = plt.subplots(1, col, figsize=(size1, size2))\n",
    "        for i in range(col):\n",
    "            axes[i].imshow(data[i], cmap=c_map) \n",
    "            axes[i].set_axis_off() \n",
    "            axes[i].set_title(f\"Image {i}\")\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(row, col, figsize=(size1, size2))\n",
    "        axes = axes.flatten()  # Flatten the axes to make indexing easier\n",
    "        for i in range(row * col):\n",
    "            if i < len(data):  # Ensure you do not exceed the length of data\n",
    "                axes[i].imshow(data[i], cmap=c_map) \n",
    "                axes[i].set_axis_off()  \n",
    "                axes[i].set_title(f\"Image {i}\")\n",
    "            else:\n",
    "                axes[i].axis('off')  # Turn off unused axes\n",
    "\n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923641df-2e9e-44cd-881c-72c8bcadc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, patch_size = 16, embed_dim = 64):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b015bc-9e0e-4b64-9d8e-c44474ffcbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba768f-f748-47fd-849a-945e577e22c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_dir):\n",
    "        \n",
    "        self.img_dir     =    img_dir\n",
    "        self.csv_dir     =    csv_dir\n",
    "        \n",
    "        self.img_files   =    sorted([f for f in os.listdir(img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.csv_files   =    sorted([f for f in os.listdir(csv_dir) if f.endswith('.csv')]) \n",
    "        \n",
    "        assert len(self.img_files) == len(self.csv_files), \"Mismatch between image and CSV files count.\" \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        csv_path = os.path.join(self.csv_dir, self.csv_files[idx])\n",
    "\n",
    "        img         = Image.open(img_path).convert(\"RGB\")  # Ensure 3-channel RGB\n",
    "        img_array   = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        img_tensor  = torch.tensor(img_array).reshape(3, 512, 512)  # Convert to PyTorch tensor and reshape to (C, H, W)\n",
    "\n",
    "\n",
    "        csv_data   = pd.read_csv(csv_path).values  # Load as NumPy array\n",
    "        csv_tensor = torch.tensor(csv_data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "\n",
    "        return img_tensor, csv_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b723c7-4d22-4d86-a6cf-f29ea9008e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_path = \"/home/turgay/falling_objects_dataset/img_files/\"\n",
    "csv_path = \"/home/turgay/falling_objects_dataset/csv_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4e400-ce55-4e0d-b43c-7d6fb59b4db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(os.listdir(img_path)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebebff-775a-4b28-a089-6e0b46183998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ImageDataset(img_dir=img_path, csv_dir=csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e4312-ff0d-4978-a369-7f0df452e0c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_image,   transform=None):\n",
    "        \n",
    "        super(MyDataset, self).__init__()\n",
    "        \n",
    "        self.X_image     =    X_image   /  255.0\n",
    "        self.transform   =    transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img   =   torch.tensor(self.X_image[idx] ,  dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b90e3-eecb-4dbe-8aba-b73a09218ef2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop to 224x224, scale between 80% to 100%\n",
    "    transforms.RandomHorizontalFlip(p=0.5),               # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(degrees=15),                # Randomly rotate the image by ±15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28ea21-6554-409b-8e39-e08b68931abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902f374-8609-4445-9164-bd2902165923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset    =  MyDataset(X_image, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06fca6-ad57-49bc-bb59-4771bfb4b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size =  int(0.8 * len(dataset))\n",
    "val_size   =  len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39a958-3da3-4399-8b9c-9bbf22b26c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fccac7-fdf6-47bc-86d2-03d7ac6e6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc62d95-1eef-479c-96e6-7e74c1eb56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b88ad8-970a-420d-8863-c220c016b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.max(), i.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebf11f-ed32-4913-ad45-50f4c3ce67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "i  =  i.reshape(-1, 512, 512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04010559-a363-46f7-8d38-2ae55dc11a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(i[:10], 2, 5, 5, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65d07b-0861-4979-a6ee-0043c200aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529c450-2213-4159-9384-b3256975774b",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Physics Model </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761731e0-121a-4f17-bf4b-702c9e95241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_with_dynamics(csv_tensor, iterations=1000):\n",
    "\n",
    "    pygame.init()\n",
    "\n",
    "    # Initialize Pymunk\n",
    "    space = pymunk.Space()\n",
    "    space.gravity = (0, 9.8)\n",
    "\n",
    "    screen_width, screen_height = 512, 512\n",
    "    screen = pygame.Surface((screen_width, screen_height))\n",
    "    draw_options = pymunk.pygame_util.DrawOptions(screen)\n",
    "\n",
    "    # Add ground\n",
    "    ground = pymunk.Segment(space.static_body, (0, screen_height), (screen_width, screen_height), 0)\n",
    "    space.add(ground)\n",
    "\n",
    "    # Create blocks from CSV tensor\n",
    "    for block_data in csv_tensor:\n",
    "        pos_x, pos_y, width, height, mass, color_r, color_g, color_b, time = block_data\n",
    "\n",
    "        moment = pymunk.moment_for_box(mass, (width, height))\n",
    "        body = pymunk.Body(mass, moment)\n",
    "        body.position = (pos_x, pos_y)\n",
    "\n",
    "        shape       = pymunk.Poly.create_box(body, (width, height))\n",
    "        shape.color = (color_r, color_g, color_b, 1.0)  # Normalize RGBA\n",
    "        space.add(body, shape)\n",
    "\n",
    "    iteration = 0\n",
    "    while iteration <= 1000:  \n",
    "        screen.fill((255, 255, 255))  # Clear the screen to white\n",
    "        space.debug_draw(draw_options)  # Draw the physics objects\n",
    "\n",
    "        if (iteration  ==  time * 50):\n",
    "            # Capture the current screen as a numpy array\n",
    "            img_array = pygame.surfarray.array3d(screen)  # Shape: (width, height, 3)\n",
    "            img_array = np.transpose(img_array, (1, 0, 2))  # Transpose to get the shape (height, width, 3)\n",
    "            \n",
    "            # Normalize the pixel values to [0, 1]\n",
    "            img_array = img_array.astype(np.float32) / 255.0\n",
    "\n",
    "            return img_array\n",
    "            break \n",
    "        \n",
    "        space.step(1 / 60.0)  # Step the physics simulation\n",
    "        iteration += 1\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffb9c3-04ab-4b53-9451-d75bbbaabefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b55ed-44f9-48ba-b92d-ff83eb1d61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_from_parameters   =  simulate_with_dynamics(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ba3f1-49e4-44b6-84c9-b0a6c1a51059",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_from_parameters[0].min(), rec_from_parameters[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b955a-63cb-4b8d-89c4-b4163545c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "i[0].shape, rec_from_parameters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795bb50-adc1-4de9-af20-35e70881010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_imgs   =  np.zeros((2, 512, 512, 3))\n",
    "\n",
    "two_imgs[0]  =  i[0]\n",
    "two_imgs[1]  =  rec_from_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65819b72-ac6f-493d-8cf5-a42a19dbc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(two_imgs, 1, 2, 8, 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36315ae0-c7c4-4eb7-80c1-825426019d69",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Physics Model for Batches </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b98ad9-8a61-4495-8d71-faa4df155419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_batch(csv_tensors, batch_size):\n",
    "    \n",
    "    screen_width, screen_height = 512, 512\n",
    "    images = []  # To store the generated images\n",
    "\n",
    "    pygame.init()\n",
    "\n",
    "    for csv_tensor in csv_tensors:\n",
    "        \n",
    "        # Initialize Pygame and Pymunk for each tensor\n",
    "        screen = pygame.Surface((screen_width, screen_height))\n",
    "        space = pymunk.Space()\n",
    "        draw_options = pymunk.pygame_util.DrawOptions(screen)\n",
    "\n",
    "        # Add ground\n",
    "        ground = pymunk.Segment(space.static_body, (0, screen_height), (screen_width, screen_height), 0)\n",
    "        space.add(ground)\n",
    "\n",
    "        # Create blocks\n",
    "        for block_data in csv_tensor:\n",
    "            pos_x, pos_y, width, height, mass, color_r, color_g, color_b, time = block_data\n",
    "\n",
    "            # Create a dynamic body and its shape\n",
    "            moment = pymunk.moment_for_box(mass, (width, height))\n",
    "            body = pymunk.Body(mass, moment)\n",
    "            body.position = (pos_x, pos_y)\n",
    "            shape = pymunk.Poly.create_box(body, (width, height))\n",
    "\n",
    "            shape.color = (color_r, color_g, color_b, 1.0)  # Normalize RGBA\n",
    "            space.add(body, shape)\n",
    "    \n",
    "        # Simulate dynamics\n",
    "        iteration = 0\n",
    "        captured  = False  # Track whether an image is captured\n",
    "        \n",
    "        while iteration <= 1000:\n",
    "            screen.fill((255, 255, 255))  # Clear the screen to white\n",
    "            space.debug_draw(draw_options)  # Draw the physics objects\n",
    "\n",
    "            # Capture image at the specified time\n",
    "            if iteration == csv_tensor[0][-1] * 50:  # Assuming `time` is consistent across blocks\n",
    "                img_array = pygame.surfarray.array3d(screen)\n",
    "                img_array = np.transpose(img_array, (1, 0, 2))  # Transpose to (height, width, 3)\n",
    "                img_array = img_array.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "                images.append(img_array)\n",
    "                captured = True\n",
    "                break\n",
    "\n",
    "            space.step(1 / 60.0)  # Step the physics simulation\n",
    "            iteration += 1\n",
    "\n",
    "        # If no image was captured for this tensor, append a blank image\n",
    "        if not captured:\n",
    "            images.append(np.zeros((screen_height, screen_width, 3), dtype=np.float32))\n",
    "\n",
    "    pygame.quit()  # Quit Pygame after all tensors are processed\n",
    "\n",
    "    # Convert the list of images to a batch array\n",
    "    result = np.array(images).reshape(-1, 3, screen_height, screen_width)\n",
    "\n",
    "    # If the batch is smaller than `batch_size`, pad with blank images\n",
    "    if len(images) < batch_size:\n",
    "        padding = np.zeros((batch_size - len(images), 3, screen_height, screen_width), dtype=np.float32)\n",
    "        result = np.vstack((result, padding))\n",
    "\n",
    "    result = torch.tensor(result, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d69c0-8703-4e23-80d6-3c51edf31725",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f959183-8845-4652-acbe-e183d108f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_imgs   =   simulate_batch(j, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3748336-5c0f-4e80-9dc2-a8b72bf78b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d224f18-a72b-49c0-ade9-337dae421503",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_recon             =   simulate_batch(params, 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09d4dc-3f7c-45e5-bebb-01297db4eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_recon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d83fcd-0d0b-4523-ba3e-1823b99cd5f7",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Decoder : ViT </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2fc4b-2b70-44ed-9ad0-42eea0555420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads   =    n_heads\n",
    "        self.att       =    torch.nn.MultiheadAttention(embed_dim =  embed_dim,\n",
    "                                                       num_heads  =  n_heads,\n",
    "                                                       dropout    =  dropout ) \n",
    "        self.q = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = torch.nn.Linear(embed_dim, embed_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.att(x, x, x)\n",
    "        \n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f998f-ef6e-4444-bccd-558a54dd47a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention(embed_dim=256, n_heads=4, dropout=0.)(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b284f-b6ce-4549-85cf-8f7d4f29afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm   =   nn.LayerNorm(embed_dim)\n",
    "        self.fn     =   fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        \n",
    "        return self.fn(self.norm(x), **kwargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc39cae-9660-4e81-a53a-efd399e8c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm = PreNorm(embed_dim=256, fn=Attention(embed_dim=256, n_heads=4, dropout=0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8902f6-7e88-40e4-b20b-7d6ec786c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07dce0-2f8b-4272-adae-894fc75f9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, dropout = 0.):    \n",
    "        \n",
    "        super().__init__(\n",
    "            \n",
    "            nn.Linear(embed_dim, hidden_dim),       #   hidden_dim   =   2   *   embed_dim\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),       \n",
    "            nn.Dropout(dropout) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9e24c-51c9-434e-8e5c-9dfffa493011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff = FeedForward(embed_dim=256, hidden_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4ccc5-dcbf-4eaf-9ddc-2a57a1491762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654df1fb-e69e-4788-862d-e415cc75e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \n",
    "        res   =   x\n",
    "        x     =   self.fn(x, **kwargs)\n",
    "        x    +=   res\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1d03c-f1f5-4fa0-99eb-362c4a109046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residual_att = ResidualAdd(Attention(embed_dim=256, n_heads=4, dropout=0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46ac57-905f-43df-b8ae-56ec648ec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#residual_att(torch.ones((1, 196, 256))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdd627-6723-48ea-a82b-68c33e5c4ca5",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Full-Model </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fefae-2f33-4a49-a098-d1dac0d9292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch=3, img_size=224, patch_size=16, embed_dim=64, latent_dim = 64, n_layers=6, dropout=0.1, heads=8):\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels    =   ch\n",
    "        self.height      =   img_size\n",
    "        self.width       =   img_size\n",
    "        self.patch_size  =   patch_size\n",
    "        self.n_layers    =   n_layers\n",
    "        self.embed_dim   =   embed_dim\n",
    "        self.latent_dim  =   latent_dim\n",
    "        \n",
    "        \n",
    "        num_patches      =   (img_size // patch_size) ** 2\n",
    "        self.num_patches =   num_patches\n",
    "        \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        # Patching\n",
    "        self.patch_embedding = PatchEmbedding(in_channels =  ch,\n",
    "                                              patch_size  =  patch_size,\n",
    "                                              embed_dim   =  self.embed_dim)\n",
    "#-----------------------------------------------------------------------------------------------------------        \n",
    "        self.pos_embedding    =    nn.Parameter(torch.randn(1, num_patches + 1, self.embed_dim))\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        # Transformer Encoder\n",
    "        self.encoder_layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            transformer_encoder_block = nn.Sequential(\n",
    "                                            ResidualAdd(PreNorm(self.embed_dim, Attention(self.embed_dim, n_heads = heads, dropout = dropout))),\n",
    "                                            ResidualAdd(PreNorm(self.embed_dim, FeedForward(self.embed_dim, 2 * self.embed_dim, dropout = dropout)))\n",
    "                                            )\n",
    "            self.encoder_layers.append(transformer_encoder_block) #  (self.num_patches,   self.embed_dim)  \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        self.fc_encoder_mu = nn.Sequential(\n",
    "            \n",
    "                                        nn.BatchNorm1d(self.num_patches * self.embed_dim),\n",
    "                                        nn.Linear(self.num_patches * self.embed_dim,   512),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                         \n",
    "                                        nn.BatchNorm1d(512),\n",
    "                                        nn.Linear(512, 256),\n",
    "                                        nn.LeakyReLU(),\n",
    "            \n",
    "                                        nn.BatchNorm1d(256),\n",
    "                                        nn.Linear(256,  self.latent_dim),\n",
    "                                        nn.LeakyReLU()\n",
    "                                        )\n",
    "    \n",
    "        self.fc_encoder_logvar = nn.Sequential(\n",
    "            \n",
    "                                        nn.BatchNorm1d(self.num_patches * self.embed_dim),\n",
    "                                        nn.Linear(self.num_patches * self.embed_dim,   512),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                         \n",
    "                                        nn.BatchNorm1d(512),\n",
    "                                        nn.Linear(512, 256),\n",
    "                                        nn.LeakyReLU(),\n",
    "            \n",
    "                                        nn.BatchNorm1d(256),\n",
    "                                        nn.Linear(256,  self.latent_dim),\n",
    "                                        nn.LeakyReLU()\n",
    "                                        )\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            \n",
    "                                        nn.Linear(self.latent_dim,         self.latent_dim * 2),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(self.latent_dim*2),\n",
    "            \n",
    "                                        nn.Linear(self.latent_dim*2,         self.latent_dim),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.BatchNorm1d(self.latent_dim),\n",
    "            \n",
    "                                        nn.Linear(self.latent_dim,        45),\n",
    "                                        nn.Softplus() \n",
    "                                        )\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "#-------------------------------------------           FUNCTIONS               ----------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "        x          =   self.patch_embedding(x)\n",
    "        b, n, _    =   x.shape \n",
    "\n",
    "        x         +=   self.pos_embedding[:, : n]\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encoder_layers[i](x)\n",
    "            \n",
    "        x      =   x.reshape(-1, self.num_patches * self.embed_dim)\n",
    "            \n",
    "        mu     = self.fc_encoder_mu(x)\n",
    "        logvar = self.fc_encoder_logvar(x) \n",
    "        \n",
    "        return mu , logvar\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) \n",
    "        z   = mu + eps * std \n",
    "        \n",
    "        return z  \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "      \n",
    "    def decode(self, z):\n",
    "        \n",
    "            x      =   self.fc_decoder(z)\n",
    "            x      =   x.reshape(-1, 5, 9)\n",
    "            \n",
    "            return x\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar        =    self.encode(x)\n",
    "        z                 =    self.reparameterize(mu, logvar)\n",
    "        params            =    self.decode(z) \n",
    "\n",
    "        kl_loss           =    -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=-1)\n",
    "        \n",
    "        \n",
    "        return params.reshape(-1, 5, 9) , kl_loss.mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337447-108f-4006-8655-b6d157519121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model   =   PINN(ch=3, img_size=512, patch_size=16, embed_dim=64, latent_dim = 64, n_layers=6, dropout=0.1, heads=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5adaae-579a-4b9d-b464-05860d5493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mu, logvar      =    model.encode(torch.randn(10, 3, 512, 512).to(device) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162dc51-dcec-4ee2-9637-b31b2a82fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z               =    model.reparameterize(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386d093-a377-44e7-a561-63ba4973bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0f30a-ade4-4626-82a7-98f3a17ec50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params            =    model.decode(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea54d2c-924d-476b-b48b-113a24b92281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e233f6-a9b1-41f9-80ef-07cd2756368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309790a-37c9-4097-b766-00f3d16aabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89218f19-68c4-4e7e-82bc-90fcc784739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model   =   PINN(ch=3, img_size=512, patch_size=16, embed_dim=64, latent_dim = 64, n_layers=6, dropout=0.1, heads=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c189e0-4186-4036-9966-0dd037381dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7c3e7-9df0-40cf-934f-c4519c6fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32a223-6d87-4d99-877c-9e8c904772b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef27a58-d9d8-474c-8310-00f41a7055be",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec40c1-0861-40ea-bab4-339048a94b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ff94-9bf5-4e36-ba65-becc198a0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model   =  \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Final_Report/codes/weights/PINN_transformers_weights_0.pth\"\n",
    "path_losses  =  \"/home/turgay/Turgay/Academic/2024-2025/Fall/Generative_Models/Final_Project/Final_Report/codes/weights/PINN_transformers_losses_0.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5235f-1ed2-4096-936d-882ab1b515e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss'       : 9999999999999,\n",
    "        }, path_model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135b3c2-dc1d-42fb-a09f-4cd8b3c868c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'train_loss_rec'   : [],\n",
    "            'train_loss_kl'    : [],\n",
    "            'train_loss_param' : [],\n",
    "            'train_loss'       : [],\n",
    "    \n",
    "            'val_loss_rec'     : [],\n",
    "            'val_loss_kl'      : [],\n",
    "            'val_loss_param'   : [],\n",
    "            'val_loss'         : [],\n",
    "    \n",
    "            'epochs'           : [],\n",
    "    \n",
    "        }, path_losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529781-d6ca-4e7e-a68c-a8abf8255a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_weights   =   torch.load(path_model, weights_only=True) \n",
    "checkpoint_losses    =   torch.load(path_losses, weights_only=True)  \n",
    "\n",
    "model.load_state_dict(checkpoint_weights['model_state_dict'])\n",
    "\n",
    "best_loss   =  checkpoint_weights['best_loss'] \n",
    "\n",
    "train_loss         =  checkpoint_losses['train_loss']\n",
    "train_loss_param   =  checkpoint_losses['train_loss_param']\n",
    "train_loss_rec     =  checkpoint_losses['train_loss_rec']\n",
    "train_loss_kl      =  checkpoint_losses['train_loss_kl']\n",
    "\n",
    "val_loss          =  checkpoint_losses['val_loss'] \n",
    "val_loss_param    =  checkpoint_losses['val_loss_param'] \n",
    "val_loss_rec      =  checkpoint_losses['val_loss_rec'] \n",
    "val_loss_kl       =  checkpoint_losses['val_loss_kl'] \n",
    "\n",
    "epochs         =  checkpoint_losses['epochs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5038eb-c033-486f-b1c4-6d4b7136711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81c30a-c94d-4b45-9beb-43d9d7509a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience      =   10  \n",
    "counter       =   0\n",
    "#------------------------------------\n",
    "beta_start    =   0.000001\n",
    "beta_end      =   10\n",
    "\n",
    "num_epochs    =   10\n",
    "#------------------------------------\n",
    "mu_values     =   [] \n",
    "logvar_values =   [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe86e4-ca46-4b03-b328-351bd8fc7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(epoch):\n",
    "    return beta_start + ((beta_end - beta_start) / num_epochs) * epoch \n",
    "#---------------------------------------------------------------------------------#\n",
    "def exponential_schedule(epoch):\n",
    "    return beta_start * ((beta_end / beta_start) ** (epoch / num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7742c0-287a-473a-a36c-a4c80e2340c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num  =  20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3dd4a-d04b-4cb0-aa4a-2e592d68b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs   =   10\n",
    "beta         =   1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dff488-a644-4a20-8698-60a9080f2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm.tqdm(train_loader):\n",
    "\n",
    "    images, csv   = batch\n",
    "    \n",
    "    images   =  images.to(device)\n",
    "    csv      =  csv.to(device) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d269ef-438c-4a22-b74d-ef2373e43033",
   "metadata": {},
   "outputs": [],
   "source": [
    "params , kl_loss      =   model(images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349a7ae-cb64-4ea9-aa3c-4b0b3385b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.shape, kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fc08a-87e5-4f56-8199-f00bfd42796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9331b45-7e6f-4f7c-8e79-8f36940d96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_recon             =   simulate_batch(params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f21de3-01f9-48ac-ac30-ce0c9da5172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e509bd-8075-4491-99dc-67efcdd60b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581beb9-7c0b-4469-892e-f62accc72e7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    alpha   =   1000\n",
    "    beta    =   0.0001     \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss_train      =   0.0\n",
    "    total_loss_train_kl   =   0.0\n",
    "    total_loss_train_rec  =   0.0\n",
    "    total_loss_train_prm  =   0.0\n",
    "\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "\n",
    "        images, csv   = batch\n",
    "        \n",
    "        images   =  images.to(device)\n",
    "        csv      =  csv.to(device) \n",
    "  \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "        \n",
    "            params , kl_loss      =   model(images) \n",
    "            \n",
    "            img_recon             =   simulate_batch(params, images.shape[0]).to(device) \n",
    "            \n",
    "            img_rec_loss          =   criterion(img_recon, images)\n",
    "            param_loss            =   criterion(csv, params) \n",
    "\n",
    "            \n",
    "\n",
    "            loss    =    img_rec_loss   +   param_loss * alpha   +    kl_loss * beta   \n",
    "            \n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "        scaler.scale(loss).backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss_train      +=   loss.item()\n",
    "        total_loss_train_kl   +=   kl_loss.item()\n",
    "        total_loss_train_rec  +=   img_rec_loss.item()  \n",
    "        total_loss_train_prm  +=   param_loss.item()\n",
    "\n",
    "    total_loss_train     /= len(train_loader)\n",
    "    total_loss_train_kl  /= len(train_loader)\n",
    "    total_loss_train_rec /= len(train_loader)\n",
    "    total_loss_train_prm /= len(train_loader) \n",
    "    \n",
    "    train_loss.append(total_loss_train)\n",
    "    train_loss_kl.append(total_loss_train_kl)\n",
    "    train_loss_rec.append(total_loss_train_rec)\n",
    "    train_loss_prm.append(total_loss_train_prm)\n",
    "    \n",
    "    total_loss_train_tensor = torch.tensor(total_loss_train)\n",
    "\n",
    "    if torch.isnan(total_loss_train_tensor):\n",
    "        print(\"nan value is encountered !\")\n",
    "\n",
    "        break\n",
    "\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss : {total_loss_train:.4f}           |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total KL Loss : {total_loss_train_kl:.4f}              |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total REC Loss : {total_loss_train_rec:.4f}              |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total PRM Loss : {total_loss_train_prm:.4f}              |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss_val      =   0.0\n",
    "    total_loss_val_kl   =   0.0\n",
    "    total_loss_val_rec  =   0.0\n",
    "    total_loss_val_reg  =   0.0\n",
    "    total_loss_val_prm  =   0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            \n",
    "            images   =  batch\n",
    "        \n",
    "            images   =  images.to(device)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            params , kl_loss      =   model(images)\n",
    "            \n",
    "            img_recon             =   simulate_batch(params, images.shape[0]).to(device) \n",
    "            \n",
    "            img_rec_loss          =   criterion(img_recon, images)\n",
    "            param_loss            =   criterion(csv, params) \n",
    "\n",
    "            \n",
    "\n",
    "            loss    =    img_rec_loss   +   param_loss * alpha   +    kl_loss * beta \n",
    "          \n",
    "\n",
    "            total_loss_val      +=   loss.item()\n",
    "            total_loss_val_kl   +=   kl_loss.item()\n",
    "            total_loss_val_rec  +=   img_rec_loss.item()\n",
    "            total_loss_val_prm  +=   param_loss.item()\n",
    "\n",
    "    \n",
    "    total_loss_val     /= len(val_loader)\n",
    "    total_loss_val_kl  /= len(val_loader)\n",
    "    total_loss_val_rec /= len(val_loader)\n",
    "    total_loss_val_prm /= len(val_loader)\n",
    "    \n",
    "    val_loss.append(total_loss_val)\n",
    "    val_loss_kl.append(total_loss_val_kl)\n",
    "    val_loss_rec.append(total_loss_val_rec)\n",
    "    val_loss_prm.append(total_loss_val_prm)\n",
    "\n",
    "\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total Validation Loss : {total_loss_val:.4f}           |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total KL Loss : {total_loss_val_kl:.4f}              |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total REC Loss : {total_loss_val_rec:.4f}             |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "    print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total PRM Loss : {total_loss_val_prm:.4f}             |\")\n",
    "    print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "    if len(val_loss) >= 2:\n",
    "        \n",
    "        res   =   (  (val_loss[-2] - val_loss[-1]) / val_loss[-2] ) * 100 \n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|              Change in loss is      %   {res:.2f}                               |\")\n",
    "        print( \"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    if total_loss_val < best_loss:\n",
    "        print(\"*************...saving best model *************\")\n",
    "        best_loss = total_loss_val \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "        }, path_model)   \n",
    "\n",
    "    torch.save({\n",
    "            'train_loss_rec'   : [],\n",
    "            'train_loss_kl'    : [],\n",
    "            'train_loss_param' : [],\n",
    "            'train_loss'       : [],\n",
    "    \n",
    "            'val_loss_rec'     : [],\n",
    "            'val_loss_kl'      : [],\n",
    "            'val_loss_param'   : [],\n",
    "            'val_loss'         : [],\n",
    "    \n",
    "            'epochs'           : [],\n",
    "    \n",
    "        }, path_losses)  \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "    if (len(val_loss) >= 2) and (val_loss[-2] > val_loss[-1]):\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e905246-3e60-4882-b807-bbc5c71fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "ax.plot(train_loss, \"b-\", label=\"Train Loss\")\n",
    "ax.plot(val_loss, \"r-\", label=\"Validation Loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"MSE_Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7a7e1-8c02-4218-9cdd-44733d9aead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\tfor img in val_loader:\n",
    "\t\timg    = img.to(device)\n",
    "\t\trecon, kl  = model(img)\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccf2ec-a548-455e-9993-301cc9dd5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085b78d-0b65-450d-9ae5-ee671205580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "\n",
    "fig, ax = plt.subplots(2, 7, figsize=(15, 4))\n",
    "\n",
    "for i in range(7):\n",
    "\tax[0, i].imshow(img[i].reshape(224,224,3).cpu().numpy())\n",
    "\tax[1, i].imshow(recon[i].reshape(224, 224, 3).cpu().numpy())\n",
    "\tax[0, i].axis('OFF')\n",
    "\tax[1, i].axis('OFF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c121ea8-67e3-4547-b80f-fa252c430027",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739986a-5cfe-46f9-af8d-edc28d743481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(dpi=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.imshow(img[1].reshape(224,224,3).cpu().numpy())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d395f1-9e41-46f6-a265-8405dfae0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logvar = model.encode(img[1].reshape(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19fff8-0837-4449-88ce-75d3c8ddd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.reparameterize(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91e0a5-8f25-49fe-847f-27df52fd6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51c1ae-757d-4e79-8cdf-271982965190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_traversal(model, z, latent_dim, steps=10, range_val=3):\n",
    "\n",
    "    device = next(model.parameters()).device  \n",
    "\n",
    "    fig, axs = plt.subplots(latent_dim, steps, figsize=(steps * 2, latent_dim * 2))\n",
    "\n",
    "    for dim in range(latent_dim):\n",
    "        for step, val in enumerate(torch.linspace(-range_val, range_val, steps)):\n",
    " \n",
    "            z_traversal             =   z.clone()\n",
    "            z_traversal[0, 0, dim]  =   val \n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                recon              = model.decode(z_traversal).squeeze(0).cpu().numpy()\n",
    "                #unnormalized_recon = ((recon + 1) * (255.0 / 2)).to(torch.int).cpu().numpy()\n",
    "\n",
    "            axs[dim, step].imshow(recon.reshape(224, 224, 3))\n",
    "            axs[dim, step].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa877515-1682-46f1-a318-0fc3a60571e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_space_traversal(model, z, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4a040-b9d3-4c82-9fbf-2f5ebf1f56b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simulate distributions\n",
    "\n",
    "B, D = 1000, 128  # Batch size and latent dimension\n",
    "\n",
    "\n",
    "\n",
    "# Simulate mu and logvar\n",
    "\n",
    "mu = np.random.normal(0, 1, B)               # Mean, normal distribution\n",
    "\n",
    "logvar = np.random.normal(-1, 0.5, B)       # Log variance, skewed normal\n",
    "\n",
    "\n",
    "\n",
    "# Calculate std and epsilon\n",
    "\n",
    "std = np.exp(0.5 * logvar)                  # Standard deviation\n",
    "\n",
    "eps = np.random.normal(0, 1, B)             # Epsilon, N(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate z\n",
    "\n",
    "z = mu + eps * std\n",
    "\n",
    "\n",
    "\n",
    "# Plot distributions\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "# Plot mu\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "\n",
    "plt.hist(mu, bins=50, alpha=0.7, color='blue', density=True)\n",
    "\n",
    "plt.title('Distribution of mu (Mean)')\n",
    "\n",
    "plt.xlabel(f'Value')\n",
    "\n",
    "plt.ylabel(f'Frequency')\n",
    "\n",
    "\n",
    "\n",
    "# Plot logvar\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "\n",
    "plt.hist(logvar, bins=50, alpha=0.7, color='orange', density=True)\n",
    "\n",
    "plt.title(f'Distribution of logvar (Log-Variance)')\n",
    "\n",
    "plt.xlabel(f'Value')\n",
    "\n",
    "plt.ylabel(f'Frequency')\n",
    "\n",
    "\n",
    "\n",
    "# Plot std\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "\n",
    "plt.hist(std, bins=50, alpha=0.7, color='green', density=True)\n",
    "\n",
    "plt.title(f'Distribution of sigma (Standard Deviation)')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "# Plot eps\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "\n",
    "plt.hist(eps, bins=50, alpha=0.7, color='red', density=True)\n",
    "\n",
    "plt.title(f'Distribution of epsilon (Random Noise)')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "# Plot z\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "\n",
    "plt.hist(z, bins=50, alpha=0.7, color='purple', density=True)\n",
    "\n",
    "plt.title(f'Distribution of z (Latent Variable)')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592a3b2-7ad4-42f4-9a88-ccbb849b719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Softplus\n",
    "softplus = nn.Softplus()\n",
    "\n",
    "# Generate input data\n",
    "x = torch.linspace(-10, 10, 100)\n",
    "y = softplus(x)\n",
    "\n",
    "# Plot Softplus\n",
    "plt.plot(x.numpy(), y.numpy(), label='Softplus(x)')\n",
    "plt.title(\"Softplus Activation Function\")\n",
    "plt.xlabel(\"Input (x)\")\n",
    "plt.ylabel(\"Output (Softplus(x))\")\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e27874-769f-4243-9902-61ab50b1c990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
